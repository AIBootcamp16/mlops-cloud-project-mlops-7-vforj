
"""
Continuous Deployment Pipeline with PyTorch for Weather Prediction
Simplified version matching project dependencies
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score
import joblib
import boto3
from botocore.exceptions import ClientError
import json
import os
from datetime import datetime
from loguru import logger
import fire

# PyTorch imports
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# Configure logger
logger.add("cd_pipeline_pytorch_{time}.log", rotation="10 MB")


class WeatherDataset(Dataset):
    """PyTorch Dataset for weather data"""
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y).reshape(-1, 1)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


class WeatherNet(nn.Module):
    """Neural Network for weather prediction"""
    def __init__(self, input_size, hidden_sizes=[64, 32, 16]):
        super(WeatherNet, self).__init__()
        
        layers = []
        prev_size = input_size
        
        # Hidden layers
        for hidden_size in hidden_sizes:
            layers.append(nn.Linear(prev_size, hidden_size))
            layers.append(nn.ReLU())
            layers.append(nn.BatchNorm1d(hidden_size))
            layers.append(nn.Dropout(0.2))
            prev_size = hidden_size
        
        # Output layer
        layers.append(nn.Linear(prev_size, 1))
        layers.append(nn.Sigmoid())
        
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.network(x)


class PyTorchModelPipeline:
    """CD Pipeline for PyTorch Weather Model"""
    
    def __init__(self, csv_path='processed_weather_data.csv', 
                 bucket_name='weather-model-bucket',
                 device='cpu'):
        self.csv_path = csv_path
        self.bucket_name = bucket_name
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.scaler = None
        self.feature_cols = None
        
        logger.info(f"Using device: {self.device}")
    
    def load_and_prepare_data(self):
        """Load and preprocess data"""
        logger.info(f"Loading data from {self.csv_path}")
        df = pd.read_csv(self.csv_path)
        
        # Define features
        possible_features = ['temperature', 'precipitation', 'wind_speed', 
                           'humidity', 'pressure', 'visibility']
        self.feature_cols = [col for col in possible_features if col in df.columns]
        target_col = 'good_to_walk' if 'good_to_walk' in df.columns else 'target'
        
        logger.info(f"Features: {self.feature_cols}")
        logger.info(f"Target: {target_col}")
        
        # Handle missing values
        df = df.dropna(subset=self.feature_cols + [target_col])
        
        X = df[self.feature_cols].values
        y = df[target_col].values
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # Scale features
        self.scaler = StandardScaler()
        X_train = self.scaler.fit_transform(X_train)
        X_test = self.scaler.transform(X_test)
        
        logger.info(f"Train samples: {len(X_train)}, Test samples: {len(X_test)}")
        logger.info(f"Class distribution - Train: {np.bincount(y_train)}")
        
        return X_train, X_test, y_train, y_test
    
    def train_model(self, X_train, y_train, X_test, y_test, 
                   epochs=100, batch_size=32, lr=0.001):
        """Train PyTorch model"""
        logger.info("Initializing PyTorch model...")
        
        input_size = X_train.shape[1]
        self.model = WeatherNet(input_size).to(self.device)
        
        # Loss and optimizer
        criterion = nn.BCELoss()
        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-5)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=10, verbose=True
        )
        
        # Create datasets and dataloaders
        train_dataset = WeatherDataset(X_train, y_train)
        test_dataset = WeatherDataset(X_test, y_test)
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, 
                                 shuffle=True, drop_last=True)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, 
                                shuffle=False)
        
        logger.info(f"Training for {epochs} epochs...")
        
        best_loss = float('inf')
        patience_counter = 0
        max_patience = 15
        
        for epoch in range(epochs):
            # Training phase
            self.model.train()
            train_loss = 0.0
            
            for batch_X, batch_y in train_loader:
                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)
                
                optimizer.zero_grad()
                outputs = self.model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
            
            train_loss /= len(train_loader)
            
            # Validation phase
            self.model.eval()
            val_loss = 0.0
            
            with torch.no_grad():
                for batch_X, batch_y in test_loader:
                    batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)
                    outputs = self.model(batch_X)
                    loss = criterion(outputs, batch_y)
                    val_loss += loss.item()
            
            val_loss /= len(test_loader)
            scheduler.step(val_loss)
            
            # Logging
            if (epoch + 1) % 10 == 0:
                logger.info(f'Epoch [{epoch+1}/{epochs}] - '
                          f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')
            
            # Early stopping
            if val_loss < best_loss:
                best_loss = val_loss
                patience_counter = 0
            else:
                patience_counter += 1
                if patience_counter >= max_patience:
                    logger.info(f"Early stopping at epoch {epoch+1}")
                    break
        
        logger.success("Model training completed")
        return self.model
    
    def evaluate_model(self, X_test, y_test):
        """Evaluate model performance"""
        logger.info("Evaluating model...")
        
        self.model.eval()
        X_tensor = torch.FloatTensor(X_test).to(self.device)
        
        with torch.no_grad():
            y_pred_proba = self.model(X_tensor).cpu().numpy().flatten()
        
        y_pred = (y_pred_proba > 0.5).astype(int)
        
        accuracy = accuracy_score(y_test, y_pred)
        auc = roc_auc_score(y_test, y_pred_proba)
        
        logger.info(f"Test Accuracy: {accuracy:.4f}")
        logger.info(f"Test AUC-ROC: {auc:.4f}")
        
        print("\n=== Classification Report ===")
        print(classification_report(y_test, y_pred, 
                                   target_names=['Not Good', 'Good to Walk']))
        
        # Model architecture summary
        print("\n=== Model Architecture ===")
        print(self.model)
        print(f"\nTotal parameters: {sum(p.numel() for p in self.model.parameters()):,}")
        
        return {
            'accuracy': float(accuracy),
            'auc_roc': float(auc),
            'total_parameters': sum(p.numel() for p in self.model.parameters())
        }
    
    def save_artifacts(self, metrics):
        """Save model artifacts"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # Save PyTorch model
        model_path = f'model_pytorch_{timestamp}.pt'
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_architecture': str(self.model),
            'input_size': len(self.feature_cols)
        }, model_path)
        logger.info(f"Model saved: {model_path}")
        
        # Save scaler
        scaler_path = f'scaler_{timestamp}.pkl'
        joblib.dump(self.scaler, scaler_path)
        logger.info(f"Scaler saved: {scaler_path}")
        
        # Save metadata
        metadata = {
            'timestamp': timestamp,
            'model_type': 'PyTorch Neural Network',
            'framework': 'PyTorch',
            'features': self.feature_cols,
            'metrics': metrics,
            'model_path': model_path,
            'scaler_path': scaler_path,
            'device': str(self.device)
        }
        
        metadata_path = f'metadata_{timestamp}.json'
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        logger.info(f"Metadata saved: {metadata_path}")
        
        return model_path, scaler_path, metadata_path, metadata
    
    def upload_to_s3(self, file_path, s3_key):
        """Upload file to AWS S3"""
        try:
            s3_client = boto3.client('s3')
            s3_client.upload_file(file_path, self.bucket_name, s3_key)
            logger.success(f"Uploaded: s3://{self.bucket_name}/{s3_key}")
            return True
        except ClientError as e:
            logger.error(f"S3 upload failed: {e}")
            return False
        except Exception as e:
            logger.error(f"Unexpected error: {e}")
            return False
    
    def deploy_to_s3(self, model_path, scaler_path, metadata_path):
        """Deploy artifacts to S3"""
        logger.info("\n=== Starting S3 Deployment ===")
        
        # Upload model
        self.upload_to_s3(model_path, f'models/pytorch/{model_path}')
        
        # Upload scaler
        self.upload_to_s3(scaler_path, f'scalers/{scaler_path}')
        
        # Upload metadata
        self.upload_to_s3(metadata_path, f'metadata/{metadata_path}')
        
        logger.success("\n=== Deployment Complete ===")
        logger.info(f"Model S3 path: s3://{self.bucket_name}/models/pytorch/{model_path}")
    
    def run_pipeline(self, epochs=100, batch_size=32, lr=0.001):
        """Execute complete CD pipeline"""
        logger.info("=== PyTorch Weather Prediction CD Pipeline ===\n")
        
        try:
            # 1. Load and prepare data
            logger.info("Step 1: Loading and preparing data...")
            X_train, X_test, y_train, y_test = self.load_and_prepare_data()
            
            # 2. Train model
            logger.info("\nStep 2: Training PyTorch model...")
            self.train_model(X_train, y_train, X_test, y_test, 
                           epochs=epochs, batch_size=batch_size, lr=lr)
            
            # 3. Evaluate model
            logger.info("\nStep 3: Evaluating model...")
            metrics = self.evaluate_model(X_test, y_test)
            
            # 4. Save artifacts
            logger.info("\nStep 4: Saving artifacts...")
            model_path, scaler_path, metadata_path, metadata = self.save_artifacts(metrics)
            
            # 5. Deploy to S3
            logger.info("\nStep 5: Deploying to AWS S3...")
            self.deploy_to_s3(model_path, scaler_path, metadata_path)
            
            logger.success("\n=== Pipeline Execution Complete ===")
            return metadata
            
        except Exception as e:
            logger.error(f"Pipeline failed: {e}")
            raise


def main(csv_path='processed_weather_data.csv', 
         bucket_name='weather-model-bucket',
         device='cpu',
         epochs=100,
         batch_size=32,
         lr=0.001):
    """
    Run the PyTorch CD pipeline
    
    Usage:
        python cd_pipeline_pytorch.py
        python cd_pipeline_pytorch.py --epochs=150 --batch_size=64
        python cd_pipeline_pytorch.py --csv_path=data.csv --device=cuda
    """
    pipeline = PyTorchModelPipeline(csv_path, bucket_name, device)
    pipeline.run_pipeline(epochs=epochs, batch_size=batch_size, lr=lr)


if __name__ == "__main__":
    fire.Fire(main)
